# ğŸ¤– Companion Robot  
### An AI-Powered Interactive Robot Using Raspberry Pi and Edge LLMs

![Flutter](https://img.shields.io/badge/Flutter-Web-blue)
![Raspberry Pi](https://img.shields.io/badge/Raspberry%20Pi-3B%2B-red)
![Python](https://img.shields.io/badge/Python-Flask-yellow)
![AI](https://img.shields.io/badge/AI-LLM%20Edge-green)
![Offline](https://img.shields.io/badge/Offline-Yes-success)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

ğŸ”— **Live Web App:**  
https://flutter-ml-web.web.app

---

## ğŸ“˜ Abstract

**Companion Robot** is an AI-powered interactive robotic system designed to operate entirely offline using edge-based intelligence. Compact enough to fit on a desk, the system integrates speech recognition, computer vision, and distance sensing through a Raspberry Pi without relying on cloud infrastructure.

Voice commands are captured via an onboard microphone and processed locally using Large Language Models (LLMs). Visual perception is provided by a camera module, while spatial awareness and collision avoidance are achieved using ultrasonic distance measurement. All computation occurs on-device, ensuring low latency, enhanced privacy, and uninterrupted operation.

The robot provides real-time feedback through an integrated display and audio output. This project demonstrates how affordable hardware combined with efficient AI pipelines can enable autonomous, privacy-preserving robotic assistants.

---

## ğŸ¯ Objectives

- Enable **fully offline AI interaction**
- Achieve **real-time perception and response**
- Ensure **safe autonomous navigation**
- Provide **web-based control via Flutter**
- Maintain **modular and scalable architecture**

---

## ğŸ§  System Architecture

Companion Robot follows a **layered architecture**, ensuring clean separation of hardware interaction, processing logic, and intelligence.

### ğŸ” High-Level Data Flow Diagram
---

## ğŸ§© Architectural Layers

### 1ï¸âƒ£ Input Layer
- **Microphone:** Captures speech commands.
- **Camera Module:** Supplies real-time visual data.
- **Ultrasonic Sensor:** Computes distance using echo timing.

---

### 2ï¸âƒ£ Processing Layer
Implemented in **Python with Flask**, this layer:
- Converts speech to text
- Normalizes sensor signals
- Routes commands between UI and AI
- Maintains real-time API communication

---

### 3ï¸âƒ£ Intelligence Layer (Edge LLM)
- Runs **locally (offline)** using LM Studio
- Performs:
  - Natural language understanding
  - Scene and object analysis
  - Contextual decision-making
  - Emotion classification

---

### 4ï¸âƒ£ Output & Feedback Layer
- **Speaker:** AI-generated speech output
- **Display:** System state and responses
- **Motors:** Physical movement execution
- **Flutter Web UI:** Control, logs, and visualization

---

## ğŸ› ï¸ Hardware Components

| Component | Description |
|--------|------------|
| Raspberry Pi 4 Model B | Central processing unit |
| Camera Module | Visual perception |
| Ultrasonic Sensor (HC-SR04) | Obstacle detection |
| Microphone | Voice input |
| Speaker | Audio output |
| 3.5â€³ Touch Display | Visual feedback |
| Motor Driver + Motors | Locomotion |
| Regulated Power Supply | Stable power delivery |

---

## ğŸ’» Software Stack

- **Frontend:** Flutter (Web/Desktop)
- **Backend:** Python, Flask, Socket.IO
- **AI Engine:** LM Studio (Local LLMs)
- **Vision:** OpenCV
- **Speech:** Web Speech API / PyAudio
- **Hardware Control:** GPIO
- **OS:** Raspberry Pi OS

---

## âš™ï¸ Flask-Based Control Logic (Conceptual)

The Flask server acts as the **central coordination unit**.


Fail-safe logic ensures immediate stop or slowdown when obstacles are detected.

---

## ğŸ­ Emotion & Interaction System

- Emotion mapping from AI responses
- Fullscreen emotion playback
- Duplicate-emotion prevention
- Automatic return to idle state

This enhances human-robot interaction quality.

---

## ğŸ–¥ï¸ Flutter Web Interface

### Features
- Manual control dashboard
- AI chat interface
- Auto Mode hotkey
- Live logs & diagnostics
- Network configuration UI
- Desktop full-screen support

## ğŸ® Control Interface Overview

<div style="border:1px solid #e5e7eb; border-radius:16px; padding:18px; background:#f9fafb;">

### ğŸ•¹ï¸ Manual Control Panel

<div style="display:grid; grid-template-columns:repeat(auto-fit,minmax(180px,1fr)); gap:14px;">

<div style="border-radius:14px; padding:14px; background:white; box-shadow:0 4px 12px rgba(0,0,0,0.05);">
ğŸ”¼ **Forward**  
Move the rover forward with controlled acceleration.
</div>

<div style="border-radius:14px; padding:14px; background:white; box-shadow:0 4px 12px rgba(0,0,0,0.05);">
â—€ï¸ **Left**  
Rotate or steer left precisely.
</div>

<div style="border-radius:14px; padding:14px; background:white; box-shadow:0 4px 12px rgba(0,0,0,0.05);">
â¹ï¸ **Stop**  
Immediately halts all movement.
</div>

<div style="border-radius:14px; padding:14px; background:white; box-shadow:0 4px 12px rgba(0,0,0,0.05);">
â–¶ï¸ **Right**  
Rotate or steer right.
</div>

<div style="border-radius:14px; padding:14px; background:white; box-shadow:0 4px 12px rgba(0,0,0,0.05);">
ğŸ”½ **Reverse**  
Moves the rover backward safely.
</div>

</div>

---

### âš¡ Quick Actions

<div style="display:grid; grid-template-columns:repeat(auto-fit,minmax(200px,1fr)); gap:14px;">

<div style="border-radius:14px; padding:14px; background:#eef2ff;">
ğŸ”„ **Rotate 90Â°**  
Performs a precise quarter-turn rotation.
</div>

<div style="border-radius:14px; padding:14px; background:#eef2ff;">
ğŸ” **Rotate 180Â°**  
Reverses direction using a half-turn rotation.
</div>

<div style="border-radius:14px; padding:14px; background:#fee2e2;">
ğŸ›‘ **Emergency Stop**  
Overrides all commands and forces a safe halt.
</div>

<div style="border-radius:14px; padding:14px; background:#ecfeff;">
ğŸ¤ **Voice Command**  
Activates offline speech-based AI control.
</div>

</div>

---

### ğŸ—£ï¸ Voice Prompt System

<div style="border-left:5px solid #6366f1; padding:12px; background:#f5f7ff; border-radius:10px;">
Text prompts can be entered manually and transmitted directly to the Raspberry Pi speaker.  
Voice commands are processed **locally** using the onboard AI pipeline.
</div>

---

### ğŸ“¡ Live Telemetry Monitoring

<div style="border-left:5px solid #10b981; padding:12px; background:#ecfdf5; border-radius:10px;">
Real-time system telemetry is displayed during operation, allowing the operator to monitor rover state, diagnostics, and safety conditions while issuing commands.
</div>

</div>

> ğŸ’¡ **Design Principle:**  
> The interface is inspired by mission-command dashboards, emphasizing safety, clarity, and immediate operator awareness without reliance on keyboard input.

---
## ğŸ§© Interactive UI & System Visualization

---

## ğŸ—ºï¸ System Data Flow (Visual Diagram Card)

<div style="border-radius:18px; padding:20px; background:linear-gradient(135deg,#f8fafc,#eef2ff); border:1px solid #e5e7eb;">

**Purpose:**  
This flow ensures *real-time, offline intelligence* with minimal latency and maximum safety.
</div>

---

## ğŸ§  Auto Mode Interface

<div style="border-radius:18px; padding:18px; background:#f0f9ff; border:1px solid #bae6fd;">

### ğŸ¤– Auto Mode (AI-Driven Control)

Auto Mode enables autonomous behavior using **local AI reasoning and sensor fusion**.

**Capabilities:**
- Continuous environment scanning
- Voice-initiated task execution
- Obstacle-aware motion control
- Emotion-synchronized responses

**Safety Behavior:**
- Ultrasonic distance monitoring
- Automatic slow-down in congested areas
- Emergency stop override at all times

> Auto Mode always yields priority to manual or emergency commands.
</div>

---

## ğŸ“¡ Live Telemetry Panel

<div style="border-radius:18px; padding:18px; background:#ecfdf5; border:1px solid #bbf7d0;">

### ğŸ“Š Real-Time System Telemetry

| Metric | Description |
|------|------------|
| CPU Load | Current Raspberry Pi processing usage |
| Temperature | Live system thermal state |
| Distance (cm) | Ultrasonic obstacle measurement |
| Network Status | API connectivity health |
| Motor State | Active / Idle / Emergency Stop |

Telemetry updates continuously while commands are executed, ensuring operator awareness and system transparency.
</div>

---

## ğŸ­ Emotion System UI

<div style="border-radius:18px; padding:18px; background:#fdf4ff; border:1px solid #f5d0fe;">

### ğŸ˜Š Emotion & Expression Engine

The Companion Robot uses an **emotion-mapped response system** to enhance interaction.

**Emotion Triggers:**
- AI response classification
- System states (idle, thinking, alert)
- Voice interaction outcomes

**UI Behavior:**
- Fullscreen emotion playback
- Duplicate emotion prevention
- Automatic fallback to idle state

**Examples:**
- ğŸ˜Š *Happy* â†’ Joke or positive response  
- ğŸ¤” *Thinking* â†’ Scanning or processing  
- ğŸš¨ *Alert* â†’ Obstacle detected  

This system improves clarity, relatability, and human-robot communication.
</div>

---

## ğŸ¯ Design Philosophy

<div style="border-left:6px solid #6366f1; padding:14px; background:#eef2ff; border-radius:12px;">
The Command Center UI follows **mission-control principles** â€” prioritizing safety, clarity, and immediate feedback while eliminating unnecessary complexity.
</div>



## ğŸ”’ Safety & Reliability

- Ultrasonic obstacle avoidance
- Speed reduction near objects
- Emergency stop mechanism
- Manual override priority
- Non-blocking asynchronous execution

---

## ğŸ“¸ Application Screenshots

<table align="center">
<tr>

<td align="center" style="border:1px solid #e5e7eb; border-radius:16px; padding:12px;">
<img src="control_page..jpg" width="300"><br><br>
<b>Manual Control Panel</b><br>
Directional and emergency controls
</td>

<td align="center" style="border:1px solid #e5e7eb; border-radius:16px; padding:12px;">
<img src="chat_interface.jpg.jpg" width="300"><br><br>
<b>AI Chat Interface</b><br>
Offline command & response system
</td>

</tr>
</table>

<br>

<table align="center">
<tr>

<td align="center" style="border:1px solid #e5e7eb; border-radius:16px; padding:12px;">
<img src="auto_mode.jpg" width="300"><br><br>
<b>Auto Mode Active</b><br>
AI-driven autonomous behavior
</td>

<td align="center" style="border:1px solid #e5e7eb; border-radius:16px; padding:12px;">
<img src="camera_preview.jpg" width="300"><br><br>
<b>Camera Preview</b><br>
Real-time vision input for AI
</td>

</tr>
</table>

<br>

<table align="center">
<tr>

<td align="center" style="border:1px solid #e5e7eb; border-radius:16px; padding:12px;">
<img src="emotion_display.jpg" width="300"><br><br>
<b>Emotion Display</b><br>
AI emotion-based visual feedback
</td>

<td align="center" style="border:1px solid #e5e7eb; border-radius:16px; padding:12px;">
<img src="desktop_fullscreen.png" width="300"><br><br>
<b>Desktop Full-Screen View</b><br>
Optimized wide-screen layout
</td>

</tr>
</table>


---

## ğŸ§ª Methodology

1. Capture user input
2. Preprocess locally
3. Interpret using LLM
4. Apply safety constraints
5. Execute action
6. Provide feedback
7. Log system state

---

## ğŸš€ Applications

- Educational robotics
- Offline AI assistants
- Human-robot interaction research
- Privacy-preserving AI systems
- Smart automation demos

---

## ğŸ“ˆ Future Scope

- Follow-me mode
- Patrol / guard mode
- Face recognition (optional)
- Expanded emotion library
- Multi-language interaction

---



## ğŸ“„ Disclaimer

This project is developed for **educational and experimental purposes**.  
Proper safety measures must be followed during physical deployment.

---

â­ *Companion Robot demonstrates the feasibility of intelligent, private, and autonomous edge-AI robotics using affordable hardware.*

